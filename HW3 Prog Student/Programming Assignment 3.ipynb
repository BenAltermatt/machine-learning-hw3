{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment 3: Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this programming assignment, you will implement logistic regression to predict the sentiment of reviews that come from `imdb.com`, `amazon.com`, and `yelp.com`. Make sure the notebook is in the same folder that contains `full_set.txt`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import string\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rc('xtick', labelsize=14) \n",
    "matplotlib.rc('ytick', labelsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set consists of 3000 sentences, each labeled '1' (if it came from a positive review) or '0' (if it came from a negative review). To be consistent with our notation from lecture, we will change the negative review label to '-1'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  -1 ;  So there is no way for me to plug it in here in the US unless I go by a converter.\n",
      "Label:  1 ;  Good case, Excellent value.\n",
      "Label:  1 ;  Great for the jawbone.\n",
      "Label:  -1 ;  Tied to charger for conversations lasting more than 45 minutes.MAJOR PROBLEMS!!\n",
      "Label:  1 ;  The mic is great.\n"
     ]
    }
   ],
   "source": [
    "## Read in the data set.\n",
    "with open(\"full_set.txt\") as f:\n",
    "    content = f.readlines()\n",
    "    \n",
    "## Remove leading and trailing white space\n",
    "content = [x.strip() for x in content]\n",
    "\n",
    "## Separate the sentences from the labels\n",
    "sentences = [x.split(\"\\t\")[0] for x in content]\n",
    "labels = [x.split(\"\\t\")[1] for x in content]\n",
    "\n",
    "## Transform the labels from '0 v.s. 1' to '-1 v.s. 1'\n",
    "y = np.array(labels, dtype='int8')\n",
    "y = 2*y - 1\n",
    "\n",
    "for i in range(5):\n",
    "    print ('Label: ',y[i],'; ', sentences[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the text data\n",
    "\n",
    "To transform this prediction problem into one amenable to linear classification, we will first need to preprocess the text data. We will do four transformations:\n",
    "\n",
    "1. Remove punctuation and numbers.\n",
    "2. Transform all words to lower-case.\n",
    "3. Remove _stop words_.\n",
    "4. Convert the sentences into vectors, using a bag-of-words representation.\n",
    "\n",
    "We begin with first two steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## full_remove takes a string x and a list of characters removal_list \n",
    "## returns x with all the characters in removal_list replaced by ' '\n",
    "def full_remove(x, removal_list):\n",
    "    for w in removal_list:\n",
    "        x = x.replace(w, ' ')\n",
    "    return x\n",
    "\n",
    "## Remove digits\n",
    "digits = [str(x) for x in range(10)]\n",
    "digit_less = [full_remove(x, digits) for x in sentences]\n",
    "\n",
    "## Remove punctuation\n",
    "punc_less = [full_remove(x, list(string.punctuation)) for x in digit_less]\n",
    "\n",
    "## Make everything lower-case\n",
    "sents_lower = [x.lower() for x in punc_less]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words\n",
    "\n",
    "Stop words are words that are filtered out because they are believed to contain no useful information for the task at hand. These usually include articles such as 'a' and 'the', pronouns such as 'i' and 'they', and prepositions such 'to' and 'from'. We have put together a very small list of stop words, but these are by no means comprehensive. Feel free to use something different; for instance, larger lists can easily be found on the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Define our stop words\n",
    "stop_set = set(['the', 'a', 'an', 'i', 'he', 'she', 'they', 'to', 'of', 'it', 'from'])\n",
    "\n",
    "## Remove stop words\n",
    "sents_split = [x.split() for x in sents_lower]\n",
    "sents_processed = [\" \".join(list(filter(lambda a: a not in stop_set, x))) for x in sents_split]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do the sentences look like so far?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['so there is no way for me plug in here in us unless go by converter',\n",
       " 'good case excellent value',\n",
       " 'great for jawbone',\n",
       " 'tied charger for conversations lasting more than minutes major problems',\n",
       " 'mic is great',\n",
       " 'have jiggle plug get line up right get decent volume',\n",
       " 'if you have several dozen or several hundred contacts then imagine fun sending each them one by one',\n",
       " 'if you are razr owner you must have this',\n",
       " 'needless say wasted my money',\n",
       " 'what waste money and time']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_processed[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words\n",
    "\n",
    "In order to use linear classifiers on our data set, we need to transform our textual data into numeric data. The classical way to do this is known as the _bag of words_ representation. In this representation, each word is thought of as corresponding to a number in `{1, 2, ..., d}` where `d` is the size of our vocabulary. And each sentence is represented as a d-dimensional vector $x$, where $x_i$ is the number of times that word $i$ occurs in the sentence.\n",
    "\n",
    "To do this transformation, we will make use of the `CountVectorizer` class in `scikit-learn` (Note that this is the only time you can call an external function from scikit-learn). We will cap the number of features at 4500, meaning a word will make it into our vocabulary only if it is one of the 4500 most common words in the corpus. This is often a useful step as it can weed out spelling mistakes and words which occur too infrequently to be useful.\n",
    "\n",
    "**Task P1:** Once you get the bag-of-words representation, append a '1' to the beginning of each vector to allow our linear classifier to learn a bias term. What is the size of the resulting data_mat matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Using cached https://files.pythonhosted.org/packages/1e/7a/dbb3be0ce9bd5c8b7e3d87328e79063f8b263b2b1bfa4774cb1147bfcd3f/sklearn-0.0.tar.gz\n",
      "Collecting scikit-learn (from sklearn)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/74/273c03e5a3b9aa0881812dd36c3f5a25f92177c96219d1d110ed96673a34/scikit_learn-1.0.2-cp37-cp37m-macosx_10_13_x86_64.whl (7.8MB)\n",
      "\u001b[K    100% |████████████████████████████████| 7.9MB 5.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0 (from scikit-learn->sklearn)\n",
      "  Downloading https://files.pythonhosted.org/packages/61/cf/6e354304bcb9c6413c4e02a747b600061c21d38ba51e7e544ac7bc66aecc/threadpoolctl-3.1.0-py3-none-any.whl\n",
      "Collecting joblib>=0.11 (from scikit-learn->sklearn)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/d4/3b4c8e5a30604df4c7518c562d4bf0502f2fa29221459226e140cf846512/joblib-1.2.0-py3-none-any.whl (297kB)\n",
      "\u001b[K    100% |████████████████████████████████| 307kB 15.2MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14.6 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.21.3)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.7.1)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn, sklearn\n",
      "  Running setup.py install for sklearn ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed joblib-1.2.0 scikit-learn-1.0.2 sklearn-0.0 threadpoolctl-3.1.0\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 22.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# I hate the python package manager so much\n",
    "\n",
    "import sys\n",
    "!{sys.executable} -m pip install sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original size:  (3000, 4500)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "## Transform to bag of words representation.\n",
    "vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None, stop_words = None, max_features = 4500)\n",
    "data_features = vectorizer.fit_transform(sents_processed)\n",
    "print ('The original size: ',data_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The updated size:  (3001, 4500)\n"
     ]
    }
   ],
   "source": [
    "## STUDENT: YOUR CODE STARTS HERE\n",
    "# Task: Append '1' to the beginning of each vector.\n",
    "# Hint: You can use data_features.toarray() to transform data_features into a numpy array\n",
    "# The output should be a numpy array named data_mat\n",
    "\n",
    "matrix = data_features.toarray()\n",
    "matrix = np.insert(matrix, 0, 1, axis=0)\n",
    "\n",
    "\n",
    "data_mat = matrix# please assign a numpy array to this\n",
    "\n",
    "## STUDENT: CODE ENDS\n",
    "print ('The updated size: ',data_mat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training / test split\n",
    "\n",
    "Finally, we split the data into a training set of 2500 sentences and a test set of 500 sentences (of which 250 are positive and 250 negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data:  (2500, 4500)\n",
      "test data:  (500, 4500)\n"
     ]
    }
   ],
   "source": [
    "## Split the data into testing and training sets\n",
    "np.random.seed(0)\n",
    "test_inds = np.append(np.random.choice((np.where(y==-1))[0], 250, replace=False), np.random.choice((np.where(y==1))[0], 250, replace=False))\n",
    "train_inds = list(set(range(len(labels))) - set(test_inds))\n",
    "\n",
    "train_data = data_mat[train_inds,]\n",
    "train_labels = y[train_inds]\n",
    "\n",
    "test_data = data_mat[test_inds,]\n",
    "test_labels = y[test_inds]\n",
    "\n",
    "print(\"train data: \", train_data.shape)\n",
    "print(\"test data: \", test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fitting a logistic regression model to the training data\n",
    "\n",
    "In this section, we will implement our own logistic regression solver using gradient descent. As we have seen in the class, to learn the parameters of logistic regression, we need to perform the following optimization:\n",
    "\n",
    "$$\n",
    "{\\bf \\tilde\\theta} =\n",
    "\\underset{{\\bf \\theta}}{\\operatorname{argmin}} \\; L_\\mathcal{D}({\\bf \\theta}) =\n",
    "\\underset{{\\bf \\theta}}{\\operatorname{argmin}} \\;\\sum_{i=1}^{n} \\ln \\left( 1 + e^{-y_i \\; {\\bf \\theta}^T {\\tilde x}_i} \\right)\n",
    "$$\n",
    "where $y_i\\in\\{-1,+1\\}$ is the label, ${\\bf \\tilde\\theta}$ is the vector of coefficients:\n",
    "$$\n",
    "{\\bf \\tilde\\theta} = \\begin{bmatrix} \\theta_0 & \\theta_1 & ... & \\theta_d \\end{bmatrix}^T,\n",
    "$$\n",
    "and ${\\tilde x}$ is the \"augmented\" feature vector (of $d+1$ dimensions), where we stick a 1 in the front of the original features:\n",
    "$$\n",
    "{\\tilde x} = \\begin{bmatrix} 1 & x_1 & ... & x_d \\end{bmatrix}^T.\n",
    "$$\n",
    "\n",
    "\n",
    "There is no nice, closed-form solution like with [least-squares linear regression](http://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_pseudoinverse) so we will use [gradient descent](http://en.wikipedia.org/wiki/Gradient_descent) instead. Specifically we will use batch gradient descent which calculates the gradient from all data points in the data set. Luckily, the loss function $L_\\mathcal{D}({\\bf \\tilde\\theta})$ we want to minimize is [convex](http://en.wikipedia.org/wiki/Convex_optimization) so there is only one minimum. Thus the minimum we arrive at is the global minimum.\n",
    "\n",
    "Gradient descent is a general method and requires twice differentiability for [smoothness](http://en.wikipedia.org/wiki/Smooth_function). It updates the parameters using a first-order approximation of the error surface.\n",
    "\n",
    "$$\n",
    "{\\bf \\tilde\\theta}_{t+1} = {\\bf \\tilde\\theta}_t + \\nabla L_\\mathcal{D}({\\bf \\tilde\\theta}_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task P2:** Derive the gradient of the loss $L_\\mathcal{D}({\\bf \\tilde\\theta})$ with respect to ${\\bf \\tilde\\theta}$, namely $\\nabla L_\\mathcal{D}({\\bf \\tilde\\theta}_t)$. The answer should depend on data points $(x_i,y_i)$ for $i=1,...,n$, and the model parameter ${\\bf \\tilde\\theta}$. Make sure you get the sign correct. Also implement the function `weight_derivative`. Print the output of the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_derivative(weights, feature_matrix, labels):\n",
    "    # Input:\n",
    "    # weights: weight vector w, a numpy vector of dimension d\n",
    "    # feature_matrix: numpy array of size n by d, where n is the number of data points, and d is the feature dimension\n",
    "    # labels: true labels y, a numpy vector of dimension d, each with value -1 or +1\n",
    "    # Output:\n",
    "    # Derivative of the regression cost function with respect to the weight w, a numpy array of dimension d\n",
    "        \n",
    "    ## STUDENT: Start of code ###\n",
    "    \n",
    "    \n",
    "        \n",
    "    return \n",
    "    # End of code ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# STUDENT: PRINT THE OUTPUT AND COPY IT TO THE SOLUTION FILE\n",
    "my_weights = np.ones(data_mat.shape[1]) # a weight of all 1s\n",
    "derivative = weight_derivative(my_weights,train_data,train_labels)\n",
    "\n",
    "print (derivative[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can just use the same gradient descent algorithm that we wrote in assignment 2 to solve it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(feature_matrix, labels, initial_weights, step_size, tolerance):\n",
    "    # Gradient descent algorithm for logistic regression problem    \n",
    "    \n",
    "    # Input:\n",
    "    # feature_matrix: numpy array of size n by d, where n is the number of data points, and d is the feature dimension\n",
    "    # labels: true labels y, a numpy vector of dimension d\n",
    "    # initial_weights: initial weight vector to start with, a numpy vector of dimension d\n",
    "    # step_size: step size of update\n",
    "    # tolerance: tolerace epsilon for stopping condition\n",
    "    # Output:\n",
    "    # Weights obtained after convergence\n",
    "\n",
    "    converged = False \n",
    "    weights = np.array(initial_weights) # current iterate\n",
    "    i = 0\n",
    "    while not converged:\n",
    "        # impelementation of what the gradient descent algorithm does in every iteration\n",
    "        # Refer back to the update rule listed above: update the weight\n",
    "        i += 1\n",
    "        derivative = weight_derivative(weights, feature_matrix, labels)\n",
    "        \n",
    "        weights -= (step_size * derivative)\n",
    "        \n",
    "        # Compute the gradient magnitude:\n",
    "        \n",
    "        gradient_magnitude = np.sqrt(np.sum(derivative**2))\n",
    "        \n",
    "        # Check the stopping condition to decide whether you want to stop the iterations\n",
    "        # print (\"grad mag :\", gradient_magnitude)\n",
    "        #print (\"tolerance:\", tolerance)\n",
    "        \n",
    "        if gradient_magnitude < tolerance:\n",
    "            converged = True\n",
    "        \n",
    "        print (\"Iteration: \",i,\"gradient_magnitude: \", gradient_magnitude) # for us to check about convergence\n",
    "        \n",
    "    return(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task P3:** Specify the initial_weights, step_size, and tolerance for the function `gradient_descent`. Copy the outputs of the code to the solution file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Initialize the weights, step size and tolerance\n",
    "# Start of code\n",
    "#STUDENT: Specify the initial_weights, step_size, and tolerance\n",
    "initial_weights =  \n",
    "step_size = \n",
    "tolerance = \n",
    "# end of code\n",
    "\n",
    "# Use the regression_gradient_descent function to calculate the gradient decent and store it in the variable 'final_weights'\n",
    "final_weights = gradient_descent(train_data,train_labels, initial_weights, step_size, tolerance)\n",
    "\n",
    "# end of code\n",
    "print (\"Here are the final weights after convergence:\")\n",
    "print (final_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task P4:** Write the code to extract the y-intercept $\\theta_0$ and the rest of the parameters $\\theta= \\begin{bmatrix} \\theta_1 & ... & \\theta_d \\end{bmatrix}^T$. Copy the code and print the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## STUDENT: CODE STARTS HERE\n",
    "## Pull out the parameters (theta_0, theta) of the logistic regression model\n",
    "theta0 = \n",
    "theta = \n",
    "## STUDENT: CODE ENDS HERE\n",
    "print ('y intercept: ',theta0)\n",
    "print ('theta1 and theta2: ',theta[1],theta[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the [logistic/sigmoid function](http://en.wikipedia.org/wiki/Logistic_function) is given by\n",
    "\n",
    "$$\n",
    "f(z)=\\frac{1}{1+e^{-z}}\n",
    "$$\n",
    "\n",
    "Based on the logistic model, the posterior probability\n",
    "\n",
    "$$\n",
    "P(y\\mid{\\bf x})=f(y\\; {\\bf \\tilde \\theta}^T {\\bf \\tilde x})\n",
    "$$\n",
    "\n",
    "To make a prediction, we can simply choose the label $y\\in\\{-1,+1\\}$ with the higher posterior probability.\n",
    "\n",
    "**Task P5:** Write the code to make the prediction for a given data matrix. Report the training error and test error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_predict(feature_matrix,weights):\n",
    "# Prediction made by logistic regression   \n",
    "    \n",
    "    # Input:\n",
    "    # feature_matrix: numpy array of size n by d+1, where n is the number of data points, and d+1 is the feature dimension\n",
    "    #                 note we have included the dummy feature as the first column of the feature_matrix\n",
    "    # weights: weight vector to start with, a numpy vector of dimension d+1\n",
    "    # Output:\n",
    "    # labels: predicted labels, a numpy vector of dimension n\n",
    "    \n",
    "    ## STUDENT: YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "    return \n",
    "    ## STUDENT: CODE ENDS\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# STUDENT: copy the output of this section to the solution file\n",
    "\n",
    "## Get predictions on training and test data\n",
    "preds_train = model_predict(train_data,final_weights)\n",
    "preds_test = model_predict(test_data,final_weights)\n",
    "\n",
    "## Compute errors\n",
    "errs_train = np.sum((preds_train > 0.0) != (train_labels > 0.0))\n",
    "errs_test = np.sum((preds_test > 0.0) != (test_labels > 0.0))\n",
    "\n",
    "print \"Training error: \", float(errs_train)/len(train_labels)\n",
    "print \"Test error: \", float(errs_test)/len(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyzing the margin\n",
    "\n",
    "As discussed in the lecture, the logistic regression model produces not just classifications but also conditional probability estimates. \n",
    "\n",
    "We will say that `x` has **margin** `gamma` if (according to the logistic regression model) `Pr(y=1|x) > (1/2)+gamma` or `Pr(y=1|x) < (1/2)-gamma`. For example, if `Pr(y=1|x)` is 0.7 according to the logistic regression model, then the margin is 0.2. If `Pr(y=1|x)` is 0.15 according to the logistic regression model, then the margin is 0.35.\n",
    "\n",
    "**Task P6:** Implement the following function **margin_counts** that takes as input the learned weights $\\bf\\tilde\\theta$, the feature matrix (`feature_matrix`), and a value of `gamma`, and computes how many points in the data have margin at least `gamma`. Copy the code and the output plot (i.e., visualization of the test set's distribution of margin values) to the solution file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def margin_counts(feature_matrix, weights, gamma):\n",
    "## Return number of points for which Pr(y=1) lies in [0, 0.5 - gamma) or (0.5 + gamma, 1]\n",
    "\n",
    "    # Input:\n",
    "    # feature_matrix: numpy array of size n by d+1, where n is the number of data points, and d+1 is the feature dimension\n",
    "    #                 note we have included the dummy feature as the first column of the feature_matrix\n",
    "    # weights: weight vector to start with, a numpy vector of dimension d+1\n",
    "    # gamma: the margin value\n",
    "    # Output:\n",
    "    # number of points for which Pr(y=1) lies in [0, 0.5 - gamma) or (0.5 + gamma, 1]\n",
    "    \n",
    "    ## STUDENT: YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "    return \n",
    "\n",
    "    ## STUDENT: CODE ENDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now visualize the test set's distribution of margin values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gammas = np.arange(0,0.5,0.01)\n",
    "f = np.vectorize(lambda g: margin_counts(test_data, final_weights,g))\n",
    "plt.plot(gammas, f(gammas)/500.0, linewidth=2, color='green')\n",
    "plt.xlabel('Margin', fontsize=14)\n",
    "plt.ylabel('Fraction of points above margin', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we investigate a natural question: Are points `x` with larger margin more likely to be classified correctly?\n",
    "\n",
    "To address this, we define a function **margin_errors** that computes the fraction of points with margin at least `gamma` that are misclassified.\n",
    "\n",
    "**Task P7:** Implement the function `margin_errors` that computes the fraction of points with margin at least `gamma` that are misclassified. Copy the code and the output plot (i.e., visualization of the relationship between margin and error rate) to the solution file. What do you observe from the plot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def margin_errors(feature_matrix, labels, weights, gamma):\n",
    "## Return error of predictions that lie in intervals [0, 0.5 - gamma) and (0.5 + gamma, 1]\n",
    "\n",
    "    # Input:\n",
    "    # feature_matrix: numpy array of size n by d+1, where n is the number of data points, and d+1 is the feature dimension\n",
    "    #                 note we have included the dummy feature as the first column of the feature_matrix\n",
    "    # labels: true labels y, a numpy vector of dimension n\n",
    "    # weights: weight vector to start with, a numpy vector of dimension d+1\n",
    "    # gamma: the margin value\n",
    "    # Output:\n",
    "    # error of predictions that lie in intervals [0, 0.5 - gamma) and (0.5 + gamma, 1]\n",
    "    \n",
    "    ## STUDENT: YOUR CODE HERE\n",
    "    \n",
    "    return \n",
    "\n",
    "    ## STUDENT: YOUR CODE ENDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now visualize the relationship between margin and error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Create grid of gamma values\n",
    "gammas = np.arange(0, 0.5, 0.01)\n",
    "\n",
    "## Compute margin_errors on test data for each value of g\n",
    "f = np.vectorize(lambda g: margin_errors(test_data, test_labels,final_weights, g))\n",
    "\n",
    "## Plot the result\n",
    "plt.plot(gammas, f(gammas), linewidth=2)\n",
    "plt.ylabel('Error rate', fontsize=14)\n",
    "plt.xlabel('Margin', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Words with large influence\n",
    "\n",
    "Finally, we attempt to partially **interpret** the logistic regression model.\n",
    "\n",
    "Which words are most important in deciding whether a sentence is positive? As a first approximation to this, we simply take the words whose coefficients in $\\theta$ have the largest positive values.\n",
    "\n",
    "Likewise, we look at the words whose coefficients in $\\theta$ have the most negative values, and we think of these as influential in negative predictions.\n",
    "\n",
    "**Task P8:** Report the top 10 positive words (i.e., words with the largest positive coefficients of $\\theta$) and the top 10 negative words (i.e., words with the most negative coefficients of $\\theta$). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Convert vocabulary into a list:\n",
    "## This is a list where the i-th entry corresponds to the \n",
    "\n",
    "vocab = np.array([z[0] for z in sorted(vectorizer.vocabulary_.items(), key=lambda x:x[1])])\n",
    "\n",
    "\n",
    "## STUDENT: YOUR CODE HERE\n",
    "\n",
    "\n",
    "## STUDENT: CODE ENDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. (Bonus question) Classifiers that can abstain\n",
    "\n",
    "Suppose you are building a classifier, and can tolerate an error rate of at most some value `e`. Unfortunately, every classifier you try has a higher error than this. \n",
    "\n",
    "Therefore, you decide that the classifier is allowed to occasionally **abstain**: that is, to say *\"don't know\"*. When it actually makes a prediction, it must have error rate at most `e`. And subject to this constraint, it should abstain as infrequently as possible.\n",
    "\n",
    "How would you build an abstaining classifier of this kind, starting from a logistic regression model? To get the bonus score, you need to show the following:\n",
    "\n",
    "* A general description of the method\n",
    "* Your code implementation\n",
    "* A case study to show how you can use it in practice (including necessary plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## STUDENT: YOUR CODE HERE\n",
    "\n",
    "\n",
    "## STUDENT: CODE ENDS"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
